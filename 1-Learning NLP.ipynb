{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP learning step by step with me for preparing interview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stemming and Lemmatization\n",
    "- N-Gram, Bi-Gram etc\n",
    "- Bag of Words (BoW)\n",
    "- Term Frequency Calculation TF\n",
    "- Inverse Document Frequency IDF\n",
    "- TFIDF Term Frequency - Inverse Document Frequency\n",
    "- Word2Vec\n",
    "- Text Classification explor with Spam classification:\n",
    "- Creating the Bag of Word model-Naive baes classifier-evaluation\n",
    "- Creating the TF-IDF word model-Random Forest-evaluation\n",
    "- Creating Word2vec model-SVM-Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming:\n",
    "- **Definition**: Stemming is a crude heuristic process that chops off the ends of words in an attempt to reduce them to their base or root form. The goal is to remove derivational affixes to get the word stem.\n",
    "- **Example**:\n",
    "  - The word `\"running\"` might be stemmed to `\"run\"`.\n",
    "  - The word `\"happiness\"` might be stemmed to `\"happi\"`.\n",
    "- **Techniques**: Algorithms like the Porter Stemmer, Snowball Stemmer, or Lancaster Stemmer are commonly used.\n",
    "- **Output**: Stemming can produce non-real words since it does not consider the meaning of the word, just the structure. For example, `\"studies\"` might be stemmed to `\"studi\"`, which is not a real word.\n",
    "- **Use Case**: Stemming is faster and less complex, often used in applications where precision is less critical, such as search engines.\n",
    "\n",
    "### Lemmatization:\n",
    "- **Definition**: Lemmatization is a more sophisticated process that reduces words to their base or dictionary form, called a lemma. It considers the context and the word’s part of speech (POS) to determine the correct form.\n",
    "- **Example**:\n",
    "  - The word `\"running\"` is lemmatized to `\"run\"`.\n",
    "  - The word `\"better\"` is lemmatized to `\"good\"`.\n",
    "- **Techniques**: Lemmatization often involves using a dictionary or a look-up table, and may also require the POS tagging of words.\n",
    "- **Output**: Lemmatization produces real, dictionary-validated words, maintaining more accurate and meaningful results compared to stemming.\n",
    "- **Use Case**: Lemmatization is more precise and is used in applications where understanding the meaning of words is important, such as text analysis or natural language processing tasks.\n",
    "\n",
    "### Summary:\n",
    "- **Stemming** is a faster, more simplistic approach that might not always produce real words, but it's useful when speed is essential, and precision is less important.\n",
    "- **Lemmatization** is a more accurate approach that results in meaningful, dictionary-based words, but it is more complex and computationally expensive.\n",
    "\n",
    "For many NLP tasks where understanding and context are important, **lemmatization** is often preferred over **stemming**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Example paragraph (you can replace this with your own text)\n",
    "paragraph = \"Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the ability to analyze, understand, and generate human language. Techniques such as tokenization, stemming, and removing stopwords are commonly used to preprocess text data for machine learning models.\"\n",
    "\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "#print(sentences)\n",
    "\n",
    "# Loop through each sentence in the paragraph\n",
    "for i in range(len(sentences)):\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    # Stem each word and remove stopwords\n",
    "    words = [stemmer.stem(word) for word in words if word.lower() not in set(stopwords.words('english'))]  # Added .lower() to handle case sensitivity\n",
    "    sentences[i]=\" \".join(words)\n",
    "\n",
    "#print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmitazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer  # Import the WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()  \n",
    "\n",
    "# Example paragraph (you can replace this with your own text)\n",
    "paragraph = \"Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the ability to analyze, understand, and generate human language. Techniques such as tokenization, stemming, and removing stopwords are commonly used to preprocess text data for machine learning models.\"\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Loop through each sentence in the paragraph\n",
    "for i in range(len(sentences)):\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    # Lemmatize each word and remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in set(stopwords.words('english'))]  # Added .lower() to handle case sensitivity\n",
    "    \n",
    "    # Join the lemmatized words back into a sentence\n",
    "    sentences[i] = \" \".join(words)\n",
    "\n",
    "# The sentences list now contains the lemmatized and cleaned sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram\n",
    "- Unigram: `please | turn | your | page`\n",
    "- Bigram: `please turn | turn your | your page`\n",
    "- Trigram: `please turn your | turn your page`\n",
    "\n",
    "You can think of an N-gram as the sequence of N words, by that notion, a 2-gram (or bigram) is a two-word sequence of words like “please turn”, “turn your”, or ”your page”, and a 3-gram (or trigram) is a three-word sequence of words like “please turn your”, or “turn your page”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. \n",
    "\n",
    "### Example:\n",
    "Suppose you have the following three sentences in your dataset:\n",
    "\n",
    "- \"The cat sat on the mat.\"\n",
    "- \"The dog sat on the log.\"\n",
    "- \"The cat and the dog played together.\"\n",
    "\n",
    "### Step 1: Preprocessing\n",
    "- **Lowercasing**: Convert all text to lowercase.\n",
    "- **Tokenization**: Split each sentence into words.\n",
    "- **Removing Stopwords**: Remove common words like \"the,\" \"and,\" \"on,\" etc.\n",
    "- **Stemming/Lemmatization (Optional)**: Reduce words to their root form (e.g., \"played\" to \"play\").\n",
    "\n",
    "Let's preprocess the sentences:\n",
    "\n",
    "- \"cat sat mat\"\n",
    "- \"dog sat log\"\n",
    "- \"cat dog played together\"\n",
    "\n",
    "### Step 2: Building the Vocabulary\n",
    "We create a vocabulary (a set of unique words) from all the preprocessed sentences:\n",
    "\n",
    "- **Vocabulary**: `[\"cat\", \"sat\", \"mat\", \"dog\", \"log\", \"played\", \"together\"]`\n",
    "\n",
    "### Step 3: Creating the Bag of Words\n",
    "For each sentence, we count the occurrence of each word from the vocabulary:\n",
    "\n",
    "|            | cat | sat | mat | dog | log | played | together |\n",
    "|------------|-----|-----|-----|-----|-----|--------|----------|\n",
    "| Sentence 1 |  1  |  1  |  1  |  0  |  0  |   0    |    0     |\n",
    "| Sentence 2 |  0  |  1  |  0  |  1  |  1  |   0    |    0     |\n",
    "| Sentence 3 |  1  |  0  |  0  |  1  |  0  |   1    |    1     |\n",
    "\n",
    "### Step 4: Using the Bag of Words Model\n",
    "The matrix above represents the Bag of Words model for the given sentences. Each row corresponds to a sentence, and each column corresponds to a word from the vocabulary. The values in the matrix indicate the frequency of each word in the corresponding sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bag of Words Model\n",
    "\n",
    "import nltk\n",
    "import re  # Import the re module for regular expressions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Initialize the PorterStemmer and WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# Example paragraph (you can replace this with your own text)\n",
    "paragraph = \"Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the ability to analyze, understand, and generate human language. Techniques such as tokenization, stemming, and removing stopwords are commonly used to preprocess text data for machine learning models.\"\n",
    "\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Initialize an empty list to store the cleaned text\n",
    "corpus = []\n",
    "\n",
    "# Cleaning the text\n",
    "for i in range(len(sentences)):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()  \n",
    "    \n",
    "    # Split the sentence into words\n",
    "    review = review.split()\n",
    "    \n",
    "    # Stem each word and remove stopwords\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Join the cleaned words back into a sentence\n",
    "    review = \" \".join(review)\n",
    "    \n",
    "    # Append the cleaned sentence to the corpus\n",
    "    corpus.append(review)\n",
    "    \n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(corpus).toarray() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n",
    "\n",
    "`TF = number of repeted words in the sentence/Number of words in sentence \n",
    "\n",
    "`idf = log( (number of sentences)/(number of sentences containing words)) \n",
    "\n",
    "\n",
    "The TF-IDF (Term Frequency-Inverse Document Frequency) model is a numerical representation used in natural language processing and information retrieval. It reflects the importance of a word in a document relative to a collection of documents (corpus).\n",
    "\n",
    "### Example:\n",
    "Suppose you have the following three sentences in your dataset:\n",
    "\n",
    "- \"The cat sat on the mat.\"\n",
    "- \"The dog sat on the log.\"\n",
    "- \"The cat and the dog played together.\"\n",
    "\n",
    "### Step 1: Preprocessing\n",
    "- **Lowercasing**: Convert all text to lowercase.\n",
    "- **Tokenization**: Split each sentence into words.\n",
    "- **Removing Stopwords**: Remove common words like \"the,\" \"and,\" \"on,\" etc.\n",
    "- **Stemming/Lemmatization (Optional)**: Reduce words to their root form (e.g., \"played\" to \"play\").\n",
    "\n",
    "Let's preprocess the sentences:\n",
    "\n",
    "- \"cat sat mat\"\n",
    "- \"dog sat log\"\n",
    "- \"cat dog played together\"\n",
    "\n",
    "### Step 2: Building the Vocabulary\n",
    "We create a vocabulary (a set of unique words) from all the preprocessed sentences:\n",
    "\n",
    "- **Vocabulary**: `[\"cat\", \"sat\", \"mat\", \"dog\", \"log\", \"played\", \"together\"]`\n",
    "\n",
    "### Step 3: Creating the TF-IDF Matrix\n",
    "For each sentence, we calculate the TF-IDF score for each word from the vocabulary:\n",
    "\n",
    "|            | cat  | sat  | mat  | dog  | log  | played | together |\n",
    "|------------|------|------|------|------|------|--------|----------|\n",
    "| Sentence 1 | 0.50 | 0.50 | 0.50 | 0.00 | 0.00 |  0.00  |   0.00   |\n",
    "| Sentence 2 | 0.00 | 0.50 | 0.00 | 0.50 | 0.50 |  0.00  |   0.00   |\n",
    "| Sentence 3 | 0.38 | 0.00 | 0.00 | 0.38 | 0.00 |  0.50  |   0.50   |\n",
    "\n",
    "### Step 4: Using the TF-IDF Model\n",
    "The matrix above represents the TF-IDF model for the given sentences. Each row corresponds to a sentence, and each column corresponds to a word from the vocabulary. The values in the matrix indicate the importance of each word in the corresponding sentence, adjusted for how often the word appears across all sentences.\n",
    "\n",
    "In this matrix:\n",
    "- A higher TF-IDF score indicates that the word is more important in that sentence compared to other words in the corpus.\n",
    "- The values are calculated based on the frequency of the word in the sentence and how common the word is across all sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF-IDF Model\n",
    "\n",
    "import nltk\n",
    "import re  # Import the re module for regular expressions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Initialize the PorterStemmer and WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# Example paragraph (you can replace this with your own text)\n",
    "paragraph = \"Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the ability to analyze, understand, and generate human language. Techniques such as tokenization, stemming, and removing stopwords are commonly used to preprocess text data for machine learning models.\"\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Initialize an empty list to store the cleaned text\n",
    "corpus = []\n",
    "\n",
    "# Cleaning the text\n",
    "for i in range(len(sentences)):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()  \n",
    "    \n",
    "    # Split the sentence into words\n",
    "    review = review.split()\n",
    "    \n",
    "    # Stem each word and remove stopwords\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Join the cleaned words back into a sentence\n",
    "    review = \" \".join(review)\n",
    "    \n",
    "    # Append the cleaned sentence to the corpus\n",
    "    corpus.append(review)\n",
    "    \n",
    "# Creating the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "x = tfidf.fit_transform(corpus).toarray()  # Fit and transform the corpus into the TF-IDF model\n",
    "\n",
    "# Now 'x' contains the TF-IDF representation of the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "### Word2Vec: An Introduction\n",
    "\n",
    "**Word2Vec** is a popular word embedding technique developed by Google in 2013. Unlike traditional text representation methods like Bag of Words (BoW) and TF-IDF, which treat words as independent entities, Word2Vec captures semantic meaning by representing words as vectors in a continuous vector space. These vectors are learned from large datasets and reflect the contextual similarity between words.\n",
    "\n",
    "### How Word2Vec Works\n",
    "\n",
    "Word2Vec uses a neural network model to learn word associations from a large corpus of text. It comes in two main architectures:\n",
    "\n",
    "1. **Continuous Bag of Words (CBOW)**: Predicts a target word based on its surrounding context words.\n",
    "   - **Example**: For the sentence \"The cat sat on the mat\", the model might use the words \"The\", \"cat\", \"on\", \"the\", and \"mat\" to predict the word \"sat\".\n",
    "   \n",
    "2. **Skip-gram**: Predicts surrounding context words given a target word.\n",
    "   - **Example**: For the word \"sat\" in the sentence \"The cat sat on the mat\", the model might try to predict the words \"cat\", \"on\", \"the\", and \"mat\".\n",
    "\n",
    "\n",
    "### Key Differences Between Word2Vec, Bag of Words, and TF-IDF\n",
    "\n",
    "#### Representation:\n",
    "- **Bag of Words (BoW)**: Represents text as a sparse matrix where each word is treated as a separate feature, with each cell containing the frequency of the word in the document.\n",
    "- **TF-IDF**: Similar to BoW but weighs the frequency of words by their inverse document frequency, giving less importance to common words across the corpus.\n",
    "- **Word2Vec**: Represents each word as a dense vector of continuous numbers, capturing semantic relationships between words.\n",
    "\n",
    "#### Contextual Understanding:\n",
    "- **Bag of Words (BoW)**: Ignores the order of words and their context; it only considers whether a word is present or absent.\n",
    "- **TF-IDF**: Like BoW, it ignores word order and context but adds a layer of importance based on how common or rare a word is across documents.\n",
    "- **Word2Vec**: Considers the context in which words appear, learning relationships between words based on their usage in sentences. Words used in similar contexts will have similar vector representations.\n",
    "\n",
    "\n",
    "#### Semantics:\n",
    "- **Bag of Words (BoW) and TF-IDF**: Do not capture semantic relationships between words. \"cat\" and \"dog\" are treated as completely unrelated features.\n",
    "- **Word2Vec**: Captures semantic relationships. Words like \"cat\" and \"dog\" will have similar vector representations because they often appear in similar contexts.\n",
    "\n",
    "### Summary\n",
    "- **Bag of Words** and **TF-IDF** are simple and useful for many tasks, but they are limited in their ability to understand the context and semantics of words.\n",
    "- **Word2Vec**, on the other hand, offers a more sophisticated approach by capturing the meanings and relationships between words, making it powerful for tasks that require semantic understanding, such as sentiment analysis, recommendation systems, and more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'token': [ 0.00180011  0.00704792  0.00294342 -0.00698107  0.00771445 -0.0059875\n",
      "  0.00899754  0.00295967 -0.00401694 -0.00468779 -0.00441776 -0.00614702\n",
      "  0.00938118 -0.00265032  0.00777397 -0.00968265  0.0021101  -0.00123162\n",
      "  0.00754255 -0.00905524  0.00743497 -0.00510684 -0.00601476 -0.00565083\n",
      " -0.0033778  -0.00341271 -0.00319543 -0.00749065  0.00070782 -0.00057491\n",
      " -0.00168308  0.00375563 -0.00762226 -0.00322158  0.00515336  0.00854375\n",
      " -0.00981021  0.00719634  0.0053107  -0.00387986  0.00857519 -0.00922167\n",
      "  0.00724838  0.00536439  0.00129314 -0.00519974 -0.00417756 -0.00335714\n",
      "  0.00160647  0.00158611  0.00738947  0.00997674  0.008866   -0.00400827\n",
      "  0.0096436  -0.00063103  0.00486533  0.00254838 -0.00063097  0.00366646\n",
      " -0.00531933 -0.00575831 -0.00760225  0.00190632  0.00652441  0.00088391\n",
      "  0.00125929  0.00317206  0.00813176 -0.00769964  0.00226079 -0.00747353\n",
      "  0.00371102  0.00951099  0.00751905  0.00642801  0.00801502  0.006552\n",
      "  0.00685522  0.00868283 -0.00494795  0.00921232  0.00505872 -0.00212982\n",
      "  0.00848806  0.00508002  0.00964879  0.0028316   0.00986974  0.0011978\n",
      "  0.009128    0.00358776  0.00656633 -0.00361327  0.00679563  0.00724403\n",
      " -0.00213175 -0.00185876  0.00361381 -0.0070382 ]\n",
      "Similar words to 'token': [('model', 0.24662119150161743), ('stem', 0.11938551068305969), ('intellig', 0.11926595866680145), ('analyz', 0.11647853255271912), ('comput', 0.09612837433815002), ('text', 0.08789688348770142), ('natur', 0.08546023070812225), ('machin', 0.07617492228746414), ('field', 0.07173426449298859), ('involv', 0.05967782437801361)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re  # Import the re module for regular expressions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim.models import Word2Vec  # Import Word2Vec\n",
    "\n",
    "# Initialize the PorterStemmer and WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# Example paragraph (you can replace this with your own text)\n",
    "paragraph = \"Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the ability to analyze, understand, and generate human language. Techniques such as tokenization, stemming, and removing stopwords are commonly used to preprocess text data for machine learning models.\"\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Initialize an empty list to store the cleaned and tokenized sentences\n",
    "corpus = []\n",
    "\n",
    "# Cleaning the text\n",
    "for i in range(len(sentences)):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()  \n",
    "    \n",
    "    # Split the sentence into words\n",
    "    review = review.split()\n",
    "    \n",
    "    # Stem each word and remove stopwords\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Append the cleaned and tokenized sentence to the corpus\n",
    "    corpus.append(review)\n",
    "\n",
    "# Training the Word2Vec model\n",
    "word2vec_model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Accessing the vocabulary (Note: 'wv.vocab' is deprecated and replaced by 'wv.index_to_key')\n",
    "words = word2vec_model.wv.index_to_key\n",
    "\n",
    "# Finding the vector for a specific word\n",
    "word = 'token'\n",
    "if word in words:\n",
    "    vector = word2vec_model.wv[word]\n",
    "    print(f\"Vector for '{word}': {vector}\")\n",
    "\n",
    "    # Finding similar words to the specific word\n",
    "    similar = word2vec_model.wv.most_similar(word)\n",
    "    print(f\"Similar words to '{word}': {similar}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not found in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      9\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      6\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n",
       "3   ham  U dun say so early hor... U c already then say...      49      6\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('spam.tsv', sep = '\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "length     0\n",
       "punct      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      9\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      6\n",
       "3   ham  U dun say so early hor... U c already then say...      49      6\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      2\n",
       "6   ham  Even my brother is not like to speak with me. ...      77      2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham = df[df['label']=='ham']\n",
    "ham.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>147</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>157</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>154</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "      <td>136</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message  length  punct\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n",
       "5   spam  FreeMsg Hey there darling it's been 3 week's n...     147      8\n",
       "8   spam  WINNER!! As a valued network customer you have...     157      6\n",
       "9   spam  Had your mobile 11 months or more? U R entitle...     154      2\n",
       "11  spam  SIX chances to win CASH! From 100 to 20,000 po...     136      8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = df[df['label']=='spam']\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4825, 4), (747, 4))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham.shape, spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ham = ham.sample(spam.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of code `ham = ham.sample(spam.shape[0])` is typically used in data processing, particularly when dealing with imbalanced datasets.\n",
    "\n",
    "### Breakdown:\n",
    "- **ham**: The DataFrame or Series containing the \"ham\" (non-spam) samples.\n",
    "- **spam.shape[0]**: The number of rows in the `spam` dataset.\n",
    "- **ham.sample(spam.shape[0])**: Randomly samples `spam.shape[0]` rows from the `ham` dataset.\n",
    "\n",
    "### Full Example:\n",
    "If `spam` has 500 rows and `ham` has 1000 rows, this line of code will randomly select 500 rows from the `ham` dataset, resulting in both `ham` and `spam` having 500 rows each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((747, 4), (747, 4))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham.shape, spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127/2287010461.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = ham.append(spam, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "data = ham.append(spam, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of code `data = ham.append(spam, ignore_index=True)` is used to combine two datasets, `ham` and `spam`, into a single DataFrame.\n",
    "\n",
    "### Breakdown:\n",
    "- **ham**: The DataFrame containing the \"ham\" (non-spam) samples.\n",
    "- **spam**: The DataFrame containing the \"spam\" samples.\n",
    "- **ham.append(spam, ignore_index=True)**: Combines the `ham` and `spam` DataFrames into a single DataFrame, `data`.\n",
    "\n",
    "### Explanation:\n",
    "- **Appending Data**: The `append()` function is used to add the rows of one DataFrame (`spam`) to the end of another DataFrame (`ham`).\n",
    "- **ignore_index=True**: This parameter resets the index of the resulting DataFrame. Instead of keeping the original index values from `ham` and `spam`, it generates a new continuous index for the combined DataFrame. This is particularly useful when you want a unified index without any duplicate or missing indices.\n",
    "\n",
    "### Result:\n",
    "The result is a new DataFrame `data` that contains all the rows from both `ham` and `spam`, with the indices reset.\n",
    "\n",
    "### Example Use Case:\n",
    "If `ham` contains 500 rows and `spam` contains 500 rows, `data` will contain 1000 rows, with the index ranging from 0 to 999.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>spam</td>\n",
       "      <td>YOUR CHANCE TO BE ON A REALITY FANTASY SHOW ca...</td>\n",
       "      <td>151</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>ham</td>\n",
       "      <td>Gumby's has a special where a  &amp;lt;#&amp;gt; \" che...</td>\n",
       "      <td>95</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>spam</td>\n",
       "      <td>5 Free Top Polyphonic Tones call 087018728737,...</td>\n",
       "      <td>157</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>spam</td>\n",
       "      <td>25p 4 alfie Moon's Children in need song on ur...</td>\n",
       "      <td>161</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>spam</td>\n",
       "      <td>Latest News! Police station toilet stolen, cop...</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message  length  punct\n",
       "1138  spam  YOUR CHANCE TO BE ON A REALITY FANTASY SHOW ca...     151      3\n",
       "87     ham  Gumby's has a special where a  &lt;#&gt; \" che...      95      9\n",
       "862   spam  5 Free Top Polyphonic Tones call 087018728737,...     157      6\n",
       "870   spam  25p 4 alfie Moon's Children in need song on ur...     161      5\n",
       "1475  spam  Latest News! Police station toilet stolen, cop...      70      3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     747\n",
       "spam    747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ham['length'], bins = 100, alpha = 0.7, label='Ham')\n",
    "plt.hist(spam['length'], bins = 100, alpha = 0.7, label = 'Spam')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ham['punct'], bins = 100, alpha = 0.7, label='Ham')\n",
    "plt.hist(spam['punct'], bins = 100, alpha = 0.7, label = 'Spam')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Bag of Word model-Naive baes classifier-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[145   6]\n",
      " [  8 140]]\n",
      "Accuracy: 95.32%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95       151\n",
      "           1       0.96      0.95      0.95       148\n",
      "\n",
      "    accuracy                           0.95       299\n",
      "   macro avg       0.95      0.95      0.95       299\n",
      "weighted avg       0.95      0.95      0.95       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re  # Import the re module for regular expressions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Import CountVectorizer for Bag of Words model\n",
    "import pandas as pd  # Import pandas for handling DataFrames\n",
    "\n",
    "# Initialize the PorterStemmer and WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "messages=data\n",
    "\n",
    "# Cleaning the text\n",
    "for i in range(0,len(messages)):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()  \n",
    "    \n",
    "    # Split the sentence into words\n",
    "    review = review.split()\n",
    "    \n",
    "    # Stem each word and remove stopwords\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Join the cleaned words back into a sentence\n",
    "    review = \" \".join(review)\n",
    "    \n",
    "    # Append the cleaned sentence to the corpus\n",
    "    corpus.append(review)\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray() \n",
    "\n",
    "# Assuming 'messages' has a 'label' column for classification\n",
    "y = pd.get_dummies(messages['label'], drop_first=True).values  # Convert labels to dummy variables\n",
    "\n",
    "# Training and Testing\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Training the model using Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB  # Corrected import statement and class name\n",
    "\n",
    "# Initialize and train the Naive Bayes model\n",
    "spam_detect_model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = spam_detect_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report \n",
    "\n",
    "# Generate the confusion matrix\n",
    "confusion_m = confusion_matrix(y_test, y_pred)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(confusion_m)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating TF-IDF-RandomForest-Evaluaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127/3138776016.py:49: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  spam_detect_model = RandomForestClassifier(n_estimators=100, random_state=0).fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[148   3]\n",
      " [ 12 136]]\n",
      "Accuracy: 94.98%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       151\n",
      "           1       0.98      0.92      0.95       148\n",
      "\n",
      "    accuracy                           0.95       299\n",
      "   macro avg       0.95      0.95      0.95       299\n",
      "weighted avg       0.95      0.95      0.95       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re  # Import the re module for regular expressions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer for TF-IDF model\n",
    "\n",
    "\n",
    "# Initialize the PorterStemmer and WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "messages = data\n",
    "\n",
    "# Cleaning the text\n",
    "for i in range(len(messages)):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()  \n",
    "    \n",
    "    # Split the sentence into words\n",
    "    review = review.split()\n",
    "    \n",
    "    # Stem each word and remove stopwords\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Join the cleaned words back into a sentence\n",
    "    review = \" \".join(review)\n",
    "    \n",
    "    # Append the cleaned sentence to the corpus\n",
    "    corpus.append(review)\n",
    "\n",
    "# Creating the TF-IDF model\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Limit the features to 5000 for efficiency\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "\n",
    "# Assuming 'messages' has a 'label' column for classification\n",
    "y = pd.get_dummies(messages['label'], drop_first=True).values  # Convert labels to dummy variables\n",
    "\n",
    "# Training and Testing\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Training the model using Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "spam_detect_model = RandomForestClassifier(n_estimators=100, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = spam_detect_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report \n",
    "\n",
    "# Generate the confusion matrix\n",
    "confusion_m = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix and accuracy\n",
    "print(confusion_m)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[118  33]\n",
      " [ 31 117]]\n",
      "Accuracy: 78.60%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.79       151\n",
      "           1       0.78      0.79      0.79       148\n",
      "\n",
      "    accuracy                           0.79       299\n",
      "   macro avg       0.79      0.79      0.79       299\n",
      "weighted avg       0.79      0.79      0.79       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re  # Import the re module for regular expressions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.svm import SVC  # Import SVM classifier\n",
    "from gensim.models import Word2Vec  # Import Word2Vec from gensim\n",
    "import pandas as pd  # Import pandas for handling DataFrames\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report \n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "messages = data  # Assuming 'data' is your DataFrame\n",
    "\n",
    "# Cleaning the text\n",
    "for i in range(0, len(messages)):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()  \n",
    "    \n",
    "    # Split the sentence into words\n",
    "    review = review.split()\n",
    "    \n",
    "    # Stem each word and remove stopwords\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Append the cleaned sentence to the corpus\n",
    "    corpus.append(review)\n",
    "\n",
    "# Training the Word2Vec model\n",
    "word2vec_model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Generating the word vectors for each sentence in the corpus\n",
    "def get_average_word2vec(sentence, model, vector_size):\n",
    "    # Function to average word vectors for a sentence\n",
    "    #Creates an array of zeros with the same size as the word vectors. \n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        if word in model.wv.key_to_index:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "X = np.array([get_average_word2vec(sentence, word2vec_model, 100) for sentence in corpus])\n",
    "\n",
    "# Assuming 'messages' has a 'label' column for classification\n",
    "y = pd.get_dummies(messages['label'], drop_first=True).values.ravel()  # Convert labels to dummy variables and flatten\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Training the model using SVM classifier\n",
    "svm_model = SVC(kernel='linear', random_state=0)  # Initialize and train the SVM model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "confusion_m = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix and accuracy\n",
    "print(confusion_m)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 4s 154ms/step - loss: 0.6944 - accuracy: 0.4895 - val_loss: 0.6935 - val_accuracy: 0.4854\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 2s 128ms/step - loss: 0.6940 - accuracy: 0.4895 - val_loss: 0.6930 - val_accuracy: 0.5146\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 2s 128ms/step - loss: 0.6950 - accuracy: 0.5021 - val_loss: 0.6934 - val_accuracy: 0.4854\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 2s 129ms/step - loss: 0.6936 - accuracy: 0.5052 - val_loss: 0.6930 - val_accuracy: 0.5146\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 2s 131ms/step - loss: 0.6942 - accuracy: 0.5010 - val_loss: 0.6935 - val_accuracy: 0.4854\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 2s 129ms/step - loss: 0.6936 - accuracy: 0.4990 - val_loss: 0.6929 - val_accuracy: 0.5146\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 2s 129ms/step - loss: 0.6940 - accuracy: 0.4854 - val_loss: 0.6934 - val_accuracy: 0.4854\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 2s 129ms/step - loss: 0.6935 - accuracy: 0.4728 - val_loss: 0.6931 - val_accuracy: 0.5146\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 2s 134ms/step - loss: 0.6934 - accuracy: 0.4895 - val_loss: 0.6935 - val_accuracy: 0.4854\n",
      "10/10 [==============================] - 1s 30ms/step\n",
      "[[  0 151]\n",
      " [  0 148]]\n",
      "Accuracy: 49.50%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       151\n",
      "           1       0.49      1.00      0.66       148\n",
      "\n",
      "    accuracy                           0.49       299\n",
      "   macro avg       0.25      0.50      0.33       299\n",
      "weighted avg       0.25      0.49      0.33       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "messages = data  # Assuming 'data' is your DataFrame\n",
    "\n",
    "# Cleaning the text\n",
    "for i in range(0, len(messages)):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Split the sentence into words\n",
    "    review = review.split()\n",
    "    \n",
    "    # Stem each word and remove stopwords\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Append the cleaned sentence to the corpus\n",
    "    corpus.append(' '.join(review))\n",
    "\n",
    "# Tokenization and word embedding preparation\n",
    "# Step 1: Initialize the Tokenizer\n",
    "tokenizer = Tokenizer()  # Tokenizer will convert words to integer indices\n",
    "\n",
    "# Step 2: Fit the tokenizer on the corpus\n",
    "tokenizer.fit_on_texts(corpus)  # Create the vocabulary based on the corpus\n",
    "\n",
    "# Step 3: Convert texts to sequences of integers\n",
    "X = tokenizer.texts_to_sequences(corpus)  # Each sentence is converted to a list of integers\n",
    "\n",
    "# Step 4: Padding sequences to ensure uniform input length\n",
    "max_len = max(len(seq) for seq in X)  # Determine the maximum sequence length\n",
    "X = pad_sequences(X, maxlen=max_len, padding='post')  # Pad sequences with zeros at the end\n",
    "\n",
    "# Assuming 'messages' has a 'label' column for classification\n",
    "y = pd.get_dummies(messages['label'], drop_first=True).values  # Convert labels to dummy variables\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Building the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Step 5: Add Embedding layer\n",
    "# The Embedding layer turns positive integers (indexes) into dense vectors of fixed size\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,  # Size of the vocabulary\n",
    "                    output_dim=100,  # Size of the word vectors\n",
    "                    input_length=max_len))  # Length of input sequences\n",
    "\n",
    "# Step 6: Add LSTM layers with Dropout for regularization\n",
    "model.add(LSTM(units=128, return_sequences=True))  # First LSTM layer with output sequences\n",
    "model.add(Dropout(0.2))  # Dropout to prevent overfitting\n",
    "model.add(LSTM(units=128))  # Second LSTM layer\n",
    "model.add(Dropout(0.2))  # Another Dropout layer\n",
    "\n",
    "# Step 7: Add output layer for binary classification\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "\n",
    "# Step 8: Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 9: Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Step 10: Train the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Step 11: Predicting the labels for the test set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
    "\n",
    "# Evaluation\n",
    "confusion_m = confusion_matrix(y_test, y_pred)  # Confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)  # Accuracy score\n",
    "\n",
    "# Display the confusion matrix and accuracy\n",
    "print(confusion_m)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))  # Detailed classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 5s 184ms/step - loss: 0.5639 - accuracy: 0.6757 - val_loss: 0.3700 - val_accuracy: 0.8536\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 2s 137ms/step - loss: 0.2291 - accuracy: 0.9215 - val_loss: 0.1656 - val_accuracy: 0.9456\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 2s 136ms/step - loss: 0.0795 - accuracy: 0.9791 - val_loss: 0.1304 - val_accuracy: 0.9582\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 2s 135ms/step - loss: 0.0451 - accuracy: 0.9895 - val_loss: 0.1896 - val_accuracy: 0.9498\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 2s 140ms/step - loss: 0.0244 - accuracy: 0.9937 - val_loss: 0.1637 - val_accuracy: 0.9582\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 2s 135ms/step - loss: 0.0148 - accuracy: 0.9969 - val_loss: 0.1690 - val_accuracy: 0.9540\n",
      "10/10 [==============================] - 1s 31ms/step\n",
      "[[146   5]\n",
      " [ 12 136]]\n",
      "Accuracy: 94.31%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94       151\n",
      "           1       0.96      0.92      0.94       148\n",
      "\n",
      "    accuracy                           0.94       299\n",
      "   macro avg       0.94      0.94      0.94       299\n",
      "weighted avg       0.94      0.94      0.94       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "messages = data  # Assuming 'data' is your DataFrame\n",
    "\n",
    "# Cleaning the text\n",
    "for i in range(0, len(messages)):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Split the sentence into words\n",
    "    review = review.split()\n",
    "    \n",
    "    # Stem each word and remove stopwords\n",
    "    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Append the cleaned sentence to the corpus\n",
    "    corpus.append(' '.join(review))\n",
    "\n",
    "# Tokenization and word embedding preparation\n",
    "# Step 1: Initialize the Tokenizer\n",
    "tokenizer = Tokenizer()  # Tokenizer will convert words to integer indices\n",
    "\n",
    "# Step 2: Fit the tokenizer on the corpus\n",
    "tokenizer.fit_on_texts(corpus)  # Create the vocabulary based on the corpus\n",
    "\n",
    "# Step 3: Convert texts to sequences of integers\n",
    "X = tokenizer.texts_to_sequences(corpus)  # Each sentence is converted to a list of integers\n",
    "\n",
    "# Step 4: Padding sequences to ensure uniform input length\n",
    "max_len = max(len(seq) for seq in X)  # Determine the maximum sequence length\n",
    "X = pad_sequences(X, maxlen=max_len, padding='post')  # Pad sequences with zeros at the end\n",
    "\n",
    "# Assuming 'messages' has a 'label' column for classification\n",
    "y = pd.get_dummies(messages['label'], drop_first=True).values  # Convert labels to dummy variables\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Building the biLSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Step 5: Add Embedding layer\n",
    "# The Embedding layer turns positive integers (indexes) into dense vectors of fixed size\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,  # Size of the vocabulary\n",
    "                    output_dim=100,  # Size of the word vectors\n",
    "                    input_length=max_len))  # Length of input sequences\n",
    "\n",
    "# Step 6: Add Bidirectional LSTM layers with Dropout for regularization\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))  # First Bidirectional LSTM layer with output sequences\n",
    "model.add(Dropout(0.2))  # Dropout to prevent overfitting\n",
    "model.add(Bidirectional(LSTM(units=128)))  # Second Bidirectional LSTM layer\n",
    "model.add(Dropout(0.2))  # Another Dropout layer\n",
    "\n",
    "# Step 7: Add output layer for binary classification\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "\n",
    "# Step 8: Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 9: Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Step 10: Train the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Step 11: Predicting the labels for the test set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
    "\n",
    "# Evaluation\n",
    "confusion_m = confusion_matrix(y_test, y_pred)  # Confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)  # Accuracy score\n",
    "\n",
    "# Display the confusion matrix and accuracy\n",
    "print(confusion_m)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))  # Detailed classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT+Tensortflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60/60 [==============================] - 128s 2s/step - loss: 0.5673 - accuracy: 0.7626 - val_loss: 0.3260 - val_accuracy: 0.9414\n",
      "Epoch 2/3\n",
      "60/60 [==============================] - 111s 2s/step - loss: 0.1576 - accuracy: 0.9571 - val_loss: 0.2017 - val_accuracy: 0.9582\n",
      "Epoch 3/3\n",
      "60/60 [==============================] - 111s 2s/step - loss: 0.1404 - accuracy: 0.9686 - val_loss: 0.2341 - val_accuracy: 0.9707\n",
      "10/10 [==============================] - 9s 824ms/step\n",
      "Confusion Matrix:\n",
      "[[141  10]\n",
      " [  4 144]]\n",
      "Accuracy: 95.32%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       151\n",
      "           1       0.94      0.97      0.95       148\n",
      "\n",
      "    accuracy                           0.95       299\n",
      "   macro avg       0.95      0.95      0.95       299\n",
      "weighted avg       0.95      0.95      0.95       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Clean the text: keep only letters, convert to lowercase, and strip extra spaces\n",
    "    cleaned_text = re.sub('[^a-zA-Z]', ' ', text).lower().strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "data['message'] = data['message'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Initialize the BERT tokenizer\n",
    "#The BertTokenizer is used to convert text data into a format that the BERT model can understand\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 3: Tokenize the text\n",
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    # Tokenize and encode the texts, making them ready for BERT\n",
    "    encoded_data = tokenizer(\n",
    "        texts.tolist(),  #Converts the text data into a list.\n",
    "        max_length=max_len, #Ensures that all tokenized sequences have the same length by padding \n",
    "        padding='max_length', #Adds padding to make all sequences the same length.\n",
    "        return_tensors='tf',  #Returns the data in TensorFlow tensor format.\n",
    "        truncation=True  #remove white space\n",
    "    )\n",
    "    return encoded_data['input_ids'].numpy(), encoded_data['attention_mask'].numpy()\n",
    "\n",
    "# Tokenize the messages\n",
    "#This converts the text data into input_ids (the actual token IDs) \n",
    "#attention_masks (which indicate which tokens should be attended to by the model).\n",
    "input_ids, attention_masks = tokenize_texts(data['message'], tokenizer)\n",
    "\n",
    "# Step 4: Prepare the labels\n",
    "labels = pd.get_dummies(data['label'], drop_first=True).values\n",
    "\n",
    "# Step 5: Split the data into training and test sets\n",
    "X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train, y_test = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the split data back into tensors\n",
    "X_train = {\n",
    "    'input_ids': tf.convert_to_tensor(X_train_ids), \n",
    "    'attention_mask': tf.convert_to_tensor(X_train_masks)\n",
    "}\n",
    "\n",
    "X_test = {\n",
    "    'input_ids': tf.convert_to_tensor(X_test_ids), \n",
    "    'attention_mask': tf.convert_to_tensor(X_test_masks)\n",
    "}\n",
    "\n",
    "# Step 6: Load the pre-trained BERT model for classification\n",
    "#Loads a pre-trained BERT model specifically designed for sequence classification tasks.\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "# Step 7: Compile the model with an optimizer and loss function\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=2e-5), #Uses the Adam optimizer with a small learning rate for fine-tuning the model.\n",
    "    loss='binary_crossentropy',  #Uses binary cross-entropy as the loss function because this is a binary classification problem.\n",
    "    metrics=['accuracy'] #Tracks accuracy during training and evaluation.\n",
    ")\n",
    "\n",
    "# Step 8: Train the model with early stopping\n",
    "history = model.fit(\n",
    "    ##The input data and labels for training.\n",
    "    X_train, y_train, \n",
    "    #The number of times the model will go through the entire training dataset.\n",
    "    epochs=3, \n",
    "    ##The number of samples the model processes before updating its parameters.\n",
    "    batch_size=16, \n",
    "    ##Splits off 20% of the training data for validation (to monitor the model’s performance on unseen data during training).\n",
    "    validation_split=0.2,  \n",
    "    #Stops training if the validation loss doesn't improve for 3 epochs, and restores the best weights.\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Step 9: Make predictions on the test set\n",
    "#Gets the raw predictions (logits) from the model for the test set.\n",
    "y_pred_logits = model.predict(X_test)['logits']\n",
    "\n",
    "# Convert the logits to binary predictions (0 or 1) using a threshold of 0.5\n",
    "y_pred = (y_pred_logits > 0).astype(\"int32\")\n",
    "\n",
    "# Step 10: Evaluate the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT model using PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 75/75 [01:51<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.31490510448813436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [01:47<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.0910369474440813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [01:51<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.046598543239136536\n",
      "Confusion Matrix:\n",
      "[[146   5]\n",
      " [  4 144]]\n",
      "Accuracy: 96.99%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97       151\n",
      "         1.0       0.97      0.97      0.97       148\n",
      "\n",
      "    accuracy                           0.97       299\n",
      "   macro avg       0.97      0.97      0.97       299\n",
      "weighted avg       0.97      0.97      0.97       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Clean the text: keep only letters, convert to lowercase, and strip extra spaces\n",
    "    cleaned_text = re.sub('[^a-zA-Z]', ' ', text).lower().strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "data['message'] = data['message'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 3: Tokenize the text\n",
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    # Tokenize and encode the texts\n",
    "    encoded_data = tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',  # Returns the data in PyTorch tensor format\n",
    "        truncation=True\n",
    "    )\n",
    "    return encoded_data['input_ids'], encoded_data['attention_mask']\n",
    "\n",
    "# Tokenize the messages\n",
    "input_ids, attention_masks = tokenize_texts(data['message'], tokenizer)\n",
    "\n",
    "# Step 4: Prepare the labels\n",
    "#The labels are converted into one-hot encoded format using pd.get_dummies\n",
    "#and then converted into PyTorch tensors of type float32.\n",
    "labels = torch.tensor(pd.get_dummies(data['label'], drop_first=True).values, dtype=torch.float32)\n",
    "\n",
    "# Step 5: Split the data into training and test sets\n",
    "X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train, y_test = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "\n",
    "# Combine the input_ids and attention_mask for training and test sets\n",
    "train_dataset = TensorDataset(X_train_ids, X_train_masks, y_train)\n",
    "test_dataset = TensorDataset(X_test_ids, X_test_masks, y_test)\n",
    "\n",
    "# Step 6: Load the pre-trained BERT model for classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "# Step 7: Set up the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "#The BCEWithLogitsLoss is used, which combines a sigmoid layer and binary cross-entropy loss.\n",
    "loss_fn = BCEWithLogitsLoss() \n",
    "\n",
    "#step 8\n",
    "# Create a DataLoader to handle the training dataset in batches\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Training loop for a specified number of epochs\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    epoch_loss = 0.0  # Initialize the loss for the current epoch\n",
    "    \n",
    "    # Iterate over each batch in the training DataLoader\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()  # Clear the gradients of all optimized variables\n",
    "        \n",
    "        # Unpack the batch into input_ids, attention_masks, and labels\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        \n",
    "        # Forward pass: compute the model's output for the inputs\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "        \n",
    "        # Compute the loss between the model's predictions and the true labels\n",
    "        loss = loss_fn(outputs.logits.squeeze(), labels.squeeze())\n",
    "        \n",
    "        # Backward pass: compute the gradients of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model's parameters using the computed gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss for the current epoch\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Compute the average loss for the current epoch\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    \n",
    "    # Print the average loss for the current epoch\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_loss}\")\n",
    "\n",
    "# Step 9: Make predictions on the test set\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a DataLoader for the test dataset with a batch size of 16\n",
    "# The shuffle=False argument ensures that the data is not shuffled, preserving the order\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize an empty list to store all predictions\n",
    "all_preds = []\n",
    "\n",
    "# Disable gradient calculations since we're only making predictions\n",
    "with torch.no_grad():\n",
    "    # Iterate over each batch in the test DataLoader\n",
    "    for batch in test_dataloader:\n",
    "        # Unpack the batch into input_ids and attention_masks\n",
    "        # The third element (labels) is ignored since it's not needed for predictions\n",
    "        input_ids, attention_masks, _ = batch\n",
    "        \n",
    "        # Perform a forward pass through the model to get the outputs\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "        \n",
    "        # Apply the sigmoid function to the logits to convert them to probabilities\n",
    "        preds = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Extend the list of all predictions with the current batch's predictions\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "y_pred = (np.array(all_preds) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Step 10: Evaluate the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test.numpy(), y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test.numpy(), y_pred) * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test.numpy(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "### DataLoader\n",
    "\n",
    "`train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)`: This line creates a `DataLoader` to efficiently handle the training dataset in batches of size 16. The `shuffle=True` argument ensures that the data is shuffled before each epoch, helping the model generalize better.\n",
    "\n",
    "### Model in Training Mode\n",
    "\n",
    "`model.train()`: This sets the model to training mode, which is necessary for certain layers like dropout and batch normalization to behave differently during training.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "`for epoch in range(3)`: This loop runs for a specified number of epochs (in this case, 3). An epoch represents one full pass through the entire training dataset.\n",
    "\n",
    "`epoch_loss = 0.0`: Initializes a variable to accumulate the loss for the current epoch.\n",
    "\n",
    "### Batch Processing\n",
    "\n",
    "`for batch in tqdm(train_dataloader)`: This loop iterates over each batch of data in the `train_dataloader`. The `tqdm` function provides a progress bar to monitor the loop's progress.\n",
    "\n",
    "`optimizer.zero_grad()`: Clears the gradients from the previous step to prevent accumulation.\n",
    "\n",
    "`input_ids, attention_masks, labels = batch`: Unpacks the batch into `input_ids`, `attention_masks`, and `labels`.\n",
    "\n",
    "`outputs = model(input_ids, attention_mask=attention_masks)`: Performs a forward pass through the model to obtain the outputs.\n",
    "\n",
    "### Loss Calculation\n",
    "\n",
    "`loss = loss_fn(outputs.logits.squeeze(), labels.squeeze())`: Computes the loss between the model's predictions and the true labels using a predefined loss function.\n",
    "\n",
    "### Backward Pass and Optimization\n",
    "\n",
    "`loss.backward()`: Computes the gradients of the loss with respect to the model's parameters.\n",
    "\n",
    "`optimizer.step()`: Updates the model's parameters based on the computed gradients.\n",
    "\n",
    "### Loss Accumulation\n",
    "\n",
    "`epoch_loss += loss.item()`: Accumulates the loss for the current epoch to calculate the average loss later.\n",
    "\n",
    "### Average Loss Calculation and Display\n",
    "\n",
    "`avg_loss = epoch_loss / len(train_dataloader)`: Calculates the average loss for the current epoch by dividing the accumulated loss by the number of batches.\n",
    "\n",
    "`print(f\"Epoch {epoch+1} - Loss: {avg_loss}\")`: Prints the average loss for the current epoch.\n",
    "\n",
    "This code trains a model for 3 epochs on a given dataset, calculating and printing the average loss at the end of each epoch. Notably, this training process does not include early stopping, meaning the model will complete all epochs regardless of the loss trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "### Set Model to Evaluation Mode\n",
    "\n",
    "`model.eval()`: This switches the model to evaluation mode. In this mode, layers like dropout and batch normalization behave differently, as they are adjusted to work appropriately for inference rather than training.\n",
    "\n",
    "### Create DataLoader for Test Set\n",
    "\n",
    "`test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)`: This creates a `DataLoader` to handle the test dataset in batches of size 16. The `shuffle=False` argument ensures that the test data is processed in its original order, which is important if you need to maintain the sequence of predictions.\n",
    "\n",
    "### Disable Gradient Calculations\n",
    "\n",
    "`with torch.no_grad()`: This context manager disables gradient calculations, reducing memory usage and speeding up computations since gradients are not needed when making predictions.\n",
    "\n",
    "### Batch Processing\n",
    "\n",
    "`for batch in test_dataloader`: This loop iterates over each batch in the `test_dataloader`.\n",
    "\n",
    "`input_ids, attention_masks, _ = batch`: Unpacks the batch into `input_ids` and `attention_masks`. The third element (usually labels) is ignored with `_` because it's not needed for prediction.\n",
    "\n",
    "### Model Inference\n",
    "\n",
    "`outputs = model(input_ids, attention_mask=attention_masks)`: The model processes the `input_ids` and `attention_masks` through a forward pass, producing raw output scores (logits).\n",
    "\n",
    "### Convert Logits to Probabilities\n",
    "\n",
    "`preds = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()`: The logits (raw scores) are passed through the sigmoid function, which converts them to probabilities. The `.squeeze()` function removes any extra dimensions, and `.cpu().numpy()` moves the tensor from the GPU to the CPU and converts it to a NumPy array.\n",
    "\n",
    "### Store Predictions\n",
    "\n",
    "`all_preds.extend(preds)`: The predictions from the current batch are added to the `all_preds` list, which stores all predictions across all batches.\n",
    "\n",
    "This code is designed to make predictions on the test set, converting the model's raw output logits into probabilities and storing those predictions for further analysis or evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
