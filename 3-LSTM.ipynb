{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bd6001-7813-4dd9-afd1-6f8935b55086",
   "metadata": {},
   "source": [
    "# Using LSTM in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9e44c9e-56f7-4519-aa5e-cf97f5a40cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/nzeinali/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and Save Data\n",
    "# Import NLTK library and download the Gutenberg corpus\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Load the raw text of Shakespeare's Hamlet from the Gutenberg corpus\n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "# Save the raw text to a file named 'hamlet.txt'\n",
    "with open('hamlet.txt', 'w') as file:  # Open the file in write mode ('w')\n",
    "    file.write(data)  # Write the content of 'data' to the file\n",
    "\n",
    "# Step 2: Data Processing\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Convert text to sequences of integers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Pad sequences to the same length\n",
    "from sklearn.model_selection import train_test_split  # Split data into training and test sets\n",
    "\n",
    "# Load the text data from the saved 'hamlet.txt' file\n",
    "with open('hamlet.txt', 'r') as file:  # Open the file in read mode ('r')\n",
    "    text = file.read().lower()  # Convert text to lowercase for uniformity\n",
    "\n",
    "# Step 3: Tokenization and Sequence Generation\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()  \n",
    "tokenizer.fit_on_texts([text])  # Fit the tokenizer on the text to create the vocabulary\n",
    "\n",
    "# Calculate the total number of unique words in the text\n",
    "total_word = len(tokenizer.word_index) + 1  # 'word_index' gives a dictionary of words and their indices\n",
    "\n",
    "# Create input sequences for model training\n",
    "input_sequences = []\n",
    "\n",
    "# Split the text into lines and process each line individually\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]  # Convert each line into a sequence of integers\n",
    "    \n",
    "    # Generate n-gram sequences from the token list\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]  # Create a sequence that includes tokens up to the current token\n",
    "        input_sequences.append(n_gram_sequence)  # Append the n-gram sequence to the list of input sequences\n",
    "\n",
    "# Step 4: Padding Sequences\n",
    "# Ensure the correct import for pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Determine the maximum length of the sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])  # Find the length of the longest sequence\n",
    "\n",
    "# Pad sequences to ensure they all have the same length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Step 5: Create Predictors and Labels\n",
    "# Import TensorFlow for further processing and model building\n",
    "import tensorflow as tf\n",
    "\n",
    "# Split input sequences into predictors (X) and labels (y)\n",
    "x, y = input_sequences[:, :-1], input_sequences[:, -1]  # X contains all tokens except the last one, y is the last token\n",
    "\n",
    "# Step 6: One-Hot Encoding\n",
    "# Convert labels (y) to one-hot encoded format\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_word)  # One-hot encode the labels based on the total number of words\n",
    "\n",
    "# Now, 'x' and 'y' are ready to be used as input and output for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5001da54-e94d-4967-86ae-cd30934e9107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219418 (4.65 MB)\n",
      "Trainable params: 1219418 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# Now, x_train and y_train are the training data, while x_test and y_test are the test data.\n",
    "\n",
    "# Step 9: Train our LSTM RNN\n",
    "from tensorflow.keras.models import Sequential  # Import the Sequential model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout  # Import necessary layers\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()  # Initialize the Sequential model\n",
    "\n",
    "# Add an Embedding layer to convert input sequences into dense vectors of size 100\n",
    "model.add(Embedding(total_word, 100, input_length=max_sequence_len-1))  \n",
    "# total_word: Size of the vocabulary\n",
    "# 100: Dimension of the embedding vectors\n",
    "# input_length: Length of input sequences\n",
    "\n",
    "# Add the first LSTM layer with 150 units, returning sequences for the next LSTM layer\n",
    "model.add(LSTM(150, return_sequences=True))  \n",
    "# 150: Number of LSTM units\n",
    "# return_sequences=True: Return the full sequence to the next LSTM layer\n",
    "\n",
    "# Add a Dropout layer with a 20% dropout rate to reduce overfitting\n",
    "model.add(Dropout(0.2))  \n",
    "# 0.2: Fraction of input units to drop\n",
    "\n",
    "# Add a second LSTM layer with 100 units, this time returning a single vector\n",
    "model.add(LSTM(100))  \n",
    "# 100: Number of LSTM units\n",
    "\n",
    "# Add a Dense layer with 'total_word' units and a softmax activation function for classification\n",
    "model.add(Dense(total_word, activation='softmax'))  \n",
    "# total_word: Number of output classes (vocabulary size)\n",
    "# activation='softmax': Activation function for multi-class classification\n",
    "\n",
    "# Optional: Compile the model with categorical cross-entropy loss and the Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "# loss: 'categorical_crossentropy' for multi-class classification\n",
    "# optimizer: 'adam' for efficient optimization\n",
    "# metrics: 'accuracy' to evaluate the model's performance\n",
    "\n",
    "# Optional: Display the model's architecture summary\n",
    "model.summary()  \n",
    "# Summarize the model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "146a8c44-7a2b-4d8b-9cc4-d8fc5b88b3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "644/644 [==============================] - 23s 32ms/step - loss: 6.9043 - accuracy: 0.0322 - val_loss: 6.7618 - val_accuracy: 0.0319\n",
      "Epoch 2/50\n",
      "644/644 [==============================] - 19s 29ms/step - loss: 6.4664 - accuracy: 0.0386 - val_loss: 6.8622 - val_accuracy: 0.0394\n",
      "Epoch 3/50\n",
      "644/644 [==============================] - 18s 27ms/step - loss: 6.3190 - accuracy: 0.0464 - val_loss: 6.8993 - val_accuracy: 0.0492\n",
      "Epoch 4/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 6.1789 - accuracy: 0.0524 - val_loss: 6.9181 - val_accuracy: 0.0501\n",
      "Epoch 5/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 6.0557 - accuracy: 0.0556 - val_loss: 6.9480 - val_accuracy: 0.0561\n",
      "Epoch 6/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 5.9242 - accuracy: 0.0602 - val_loss: 7.0062 - val_accuracy: 0.0602\n",
      "Epoch 7/50\n",
      "644/644 [==============================] - 18s 27ms/step - loss: 5.7896 - accuracy: 0.0690 - val_loss: 7.0393 - val_accuracy: 0.0651\n",
      "Epoch 8/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 5.6559 - accuracy: 0.0760 - val_loss: 7.0954 - val_accuracy: 0.0616\n",
      "Epoch 9/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 5.5326 - accuracy: 0.0837 - val_loss: 7.1785 - val_accuracy: 0.0668\n",
      "Epoch 10/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 5.4091 - accuracy: 0.0927 - val_loss: 7.2441 - val_accuracy: 0.0697\n",
      "Epoch 11/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 5.2924 - accuracy: 0.0986 - val_loss: 7.3580 - val_accuracy: 0.0690\n",
      "Epoch 12/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 5.1694 - accuracy: 0.1036 - val_loss: 7.4209 - val_accuracy: 0.0719\n",
      "Epoch 13/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 5.0535 - accuracy: 0.1094 - val_loss: 7.5154 - val_accuracy: 0.0738\n",
      "Epoch 14/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 4.9354 - accuracy: 0.1147 - val_loss: 7.6503 - val_accuracy: 0.0742\n",
      "Epoch 15/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 4.8216 - accuracy: 0.1175 - val_loss: 7.7290 - val_accuracy: 0.0723\n",
      "Epoch 16/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 4.7063 - accuracy: 0.1260 - val_loss: 7.8834 - val_accuracy: 0.0738\n",
      "Epoch 17/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 4.5918 - accuracy: 0.1304 - val_loss: 7.9716 - val_accuracy: 0.0674\n",
      "Epoch 18/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 4.4839 - accuracy: 0.1384 - val_loss: 8.1156 - val_accuracy: 0.0663\n",
      "Epoch 19/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 4.3832 - accuracy: 0.1427 - val_loss: 8.2544 - val_accuracy: 0.0701\n",
      "Epoch 20/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 4.2801 - accuracy: 0.1502 - val_loss: 8.3489 - val_accuracy: 0.0725\n",
      "Epoch 21/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 4.1852 - accuracy: 0.1609 - val_loss: 8.5074 - val_accuracy: 0.0686\n",
      "Epoch 22/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 4.0927 - accuracy: 0.1723 - val_loss: 8.6363 - val_accuracy: 0.0715\n",
      "Epoch 23/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 4.0057 - accuracy: 0.1860 - val_loss: 8.7888 - val_accuracy: 0.0666\n",
      "Epoch 24/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 3.9183 - accuracy: 0.1951 - val_loss: 8.9351 - val_accuracy: 0.0709\n",
      "Epoch 25/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.8412 - accuracy: 0.2110 - val_loss: 9.0353 - val_accuracy: 0.0663\n",
      "Epoch 26/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 3.7671 - accuracy: 0.2200 - val_loss: 9.1534 - val_accuracy: 0.0686\n",
      "Epoch 27/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.6917 - accuracy: 0.2323 - val_loss: 9.2781 - val_accuracy: 0.0668\n",
      "Epoch 28/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.6224 - accuracy: 0.2483 - val_loss: 9.4230 - val_accuracy: 0.0643\n",
      "Epoch 29/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 3.5564 - accuracy: 0.2559 - val_loss: 9.4856 - val_accuracy: 0.0643\n",
      "Epoch 30/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.4958 - accuracy: 0.2665 - val_loss: 9.6158 - val_accuracy: 0.0653\n",
      "Epoch 31/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.4344 - accuracy: 0.2769 - val_loss: 9.7099 - val_accuracy: 0.0657\n",
      "Epoch 32/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.3824 - accuracy: 0.2872 - val_loss: 9.8117 - val_accuracy: 0.0653\n",
      "Epoch 33/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.3197 - accuracy: 0.2985 - val_loss: 9.9294 - val_accuracy: 0.0639\n",
      "Epoch 34/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 3.2696 - accuracy: 0.3049 - val_loss: 10.0136 - val_accuracy: 0.0629\n",
      "Epoch 35/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 3.2176 - accuracy: 0.3143 - val_loss: 10.1153 - val_accuracy: 0.0620\n",
      "Epoch 36/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 3.1692 - accuracy: 0.3259 - val_loss: 10.2178 - val_accuracy: 0.0626\n",
      "Epoch 37/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 3.1232 - accuracy: 0.3306 - val_loss: 10.2617 - val_accuracy: 0.0598\n",
      "Epoch 38/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.0814 - accuracy: 0.3421 - val_loss: 10.3711 - val_accuracy: 0.0589\n",
      "Epoch 39/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 3.0266 - accuracy: 0.3501 - val_loss: 10.4544 - val_accuracy: 0.0612\n",
      "Epoch 40/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 2.9865 - accuracy: 0.3600 - val_loss: 10.5308 - val_accuracy: 0.0589\n",
      "Epoch 41/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 2.9445 - accuracy: 0.3641 - val_loss: 10.6177 - val_accuracy: 0.0595\n",
      "Epoch 42/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 2.9041 - accuracy: 0.3748 - val_loss: 10.7030 - val_accuracy: 0.0606\n",
      "Epoch 43/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 2.8596 - accuracy: 0.3813 - val_loss: 10.7471 - val_accuracy: 0.0554\n",
      "Epoch 44/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 2.8165 - accuracy: 0.3906 - val_loss: 10.8042 - val_accuracy: 0.0612\n",
      "Epoch 45/50\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 2.7867 - accuracy: 0.3966 - val_loss: 10.9129 - val_accuracy: 0.0600\n",
      "Epoch 46/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 2.7484 - accuracy: 0.4039 - val_loss: 10.9963 - val_accuracy: 0.0569\n",
      "Epoch 47/50\n",
      "644/644 [==============================] - 16s 26ms/step - loss: 2.7086 - accuracy: 0.4117 - val_loss: 11.0359 - val_accuracy: 0.0552\n",
      "Epoch 48/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 2.6712 - accuracy: 0.4198 - val_loss: 11.1177 - val_accuracy: 0.0579\n",
      "Epoch 49/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 2.6409 - accuracy: 0.4238 - val_loss: 11.1855 - val_accuracy: 0.0608\n",
      "Epoch 50/50\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 2.6082 - accuracy: 0.4285 - val_loss: 11.2750 - val_accuracy: 0.0598\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train,  # Training data (features)\n",
    "    y_train,  # Training labels\n",
    "    epochs=50,  # Number of epochs (iterations over the entire dataset)\n",
    "    validation_data=(x_test, y_test),  # Validation data to evaluate model performance on unseen data\n",
    "    verbose=1  # Verbosity mode (1 shows progress bar with detailed info)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8343128c-2e09-49a9-9f33-e9ce0361d41c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nzeinali/.local/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    # Tokenize the input text and convert it to a sequence of integers\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    \n",
    "    # Ensure the sequence length matches max_sequence_len-1 (trimming if necessary)\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  \n",
    "    \n",
    "    # Pad the sequence to match the input length expected by the model\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    \n",
    "    # Predict the next word using the trained model\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    \n",
    "    # Find the index of the word with the highest probability\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    \n",
    "    # Map the index back to the corresponding word using the tokenizer\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word  # Return the predicted word\n",
    "    return None  # Return None if no word is found\n",
    "\n",
    "# Step 11: Save the model and tokenizer\n",
    "## Save the trained model to a file\n",
    "model.save(\"next_word_lstm.h5\")\n",
    "\n",
    "## Save the tokenizer to a file using pickle\n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40ba893c-deb4-4ffc-a00f-37a57191550e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: To be or not to be\n",
      "Next Word Prediction: damn'd\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Test the model with sample input text\n",
    "input_text = \"To be or not to be\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "\n",
    "# Calculate the max sequence length for prediction\n",
    "max_sequence_len = model.input_shape[1] + 1\n",
    "\n",
    "# Predict the next word for the given input text\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "print(f\"Next Word Prediction: {next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec69afc8-5317-4fe7-852d-75914b5057d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Barn. Last night of all,When yond same\n",
      "Next Word Prediction: starre\n"
     ]
    }
   ],
   "source": [
    "# more example\n",
    "# Test the model with another sample input text\n",
    "input_text = \"Barn. Last night of all,When yond same\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "\n",
    "# Calculate the max sequence length for prediction\n",
    "max_sequence_len = model.input_shape[1] + 1\n",
    "\n",
    "# Predict the next word for the given input text\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "print(f\"Next Word Prediction: {next_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a38f03-0ef1-4a7b-8e87-36b05167575a",
   "metadata": {},
   "source": [
    "# APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fc91e-8f94-4fc0-beed-65aca52588e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Load the LSTM Model\n",
    "model=load_model('next_word_lstm.h5')\n",
    "\n",
    "#3 Laod the tokenizer\n",
    "with open('tokenizer.pickle','rb') as handle:\n",
    "    tokenizer=pickle.load(handle)\n",
    "\n",
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# streamlit app\n",
    "st.title(\"Next Word Prediction With LSTM And Early Stopping\")\n",
    "input_text=st.text_input(\"Enter the sequence of Words\",\"To be or not to\")\n",
    "if st.button(\"Predict Next Word\"):\n",
    "    max_sequence_len = model.input_shape[1] + 1  # Retrieve the max sequence length from the model input shape\n",
    "    next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "    st.write(f'Next word: {next_word}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
